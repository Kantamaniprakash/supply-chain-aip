{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supply Chain AIP — Model Validation Notebook\n",
    "\n",
    "**Author:** Satya Sai Prakash Kantamani  \n",
    "**Purpose:** End-to-end model evaluation pipeline for the supply chain disruption risk prediction system.  \n",
    "**Scope:** DisruptionRiskModel (XGBoost), AnomalyDetector (IF+ECOD), and MonteCarloVaR validation.\n",
    "\n",
    "### Sections\n",
    "1. Synthetic data generation (mirrors production feature distribution)\n",
    "2. DisruptionRiskModel — CV metrics, calibration, SHAP analysis\n",
    "3. AnomalyDetector — ROC curve, precision-recall trade-off\n",
    "4. Monte Carlo VaR — convergence analysis, sensitivity to correlation structure\n",
    "5. Model Registry — drift simulation and challenger promotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "sns.set_theme(style='whitegrid', palette='deep', font_scale=1.1)\n",
    "plt.rcParams.update({'figure.dpi': 120, 'figure.figsize': (12, 5)})\n",
    "\n",
    "print('Environment ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Synthetic Data Generation\n",
    "\n",
    "Generate a realistic supplier feature dataset with correlated signals, class imbalance (~18% positive rate), and temporal structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.disruption_risk_model import FEATURES, TARGET\n",
    "\n",
    "N = 3000\n",
    "\n",
    "df = pd.DataFrame({f: np.random.rand(N) for f in FEATURES})\n",
    "\n",
    "# Correlated features (realistic)\n",
    "df['on_time_rate_90d']        = np.random.beta(8, 2, N)\n",
    "df['on_time_rate_30d']        = df['on_time_rate_90d'] * np.random.uniform(0.85, 1.15, N)\n",
    "df['on_time_rate_7d']         = df['on_time_rate_30d'] * np.random.uniform(0.80, 1.20, N)\n",
    "df['on_time_rate_ewm_alpha02']= df['on_time_rate_90d'] * 0.2 + df['on_time_rate_30d'] * 0.8\n",
    "df['disruption_rate_90d']     = np.random.beta(2, 10, N)\n",
    "df['geo_risk_score']          = np.random.uniform(0, 1, N)\n",
    "df['financial_risk_score']    = np.random.beta(3, 7, N)\n",
    "df['network_pagerank']        = np.random.exponential(0.1, N)\n",
    "df['avg_delay_days_30d']      = np.random.exponential(2, N)\n",
    "df['avg_delay_days_90d']      = df['avg_delay_days_30d'] * np.random.uniform(0.8, 1.2, N)\n",
    "df['delay_std_90d']           = df['avg_delay_days_90d'] * np.random.uniform(0.3, 1.5, N)\n",
    "df['lead_time_variance_ratio'] = df['delay_std_90d'] / (df['avg_delay_days_90d'] + 1)\n",
    "\n",
    "# Clip to valid ranges\n",
    "for col in ['on_time_rate_90d', 'on_time_rate_30d', 'on_time_rate_7d',\n",
    "            'on_time_rate_ewm_alpha02']:\n",
    "    df[col] = df[col].clip(0, 1)\n",
    "\n",
    "# Synthesise label: logistic signal with noise\n",
    "risk_signal = (\n",
    "    0.30 * df['geo_risk_score']\n",
    "    + 0.25 * df['disruption_rate_90d']\n",
    "    + 0.25 * (1 - df['on_time_rate_90d'])\n",
    "    + 0.10 * df['financial_risk_score']\n",
    "    + 0.10 * df['network_pagerank'].clip(0, 1)\n",
    ")\n",
    "df[TARGET] = (risk_signal + np.random.normal(0, 0.08, N) > 0.40).astype(int)\n",
    "df['supplier_id'] = [f'SUP-{i:04d}' for i in range(N)]\n",
    "\n",
    "# Train/test split (temporal: first 80% train, last 20% test)\n",
    "train_df = df.iloc[:int(N*0.8)].copy()\n",
    "test_df  = df.iloc[int(N*0.8):].copy()\n",
    "\n",
    "print(f'Dataset: {N:,} suppliers | Positive rate: {df[TARGET].mean():.1%}')\n",
    "print(f'Train: {len(train_df):,} | Test: {len(test_df):,}')\n",
    "df[FEATURES].describe().T[['mean','std','min','50%','max']].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DisruptionRiskModel — Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.disruption_risk_model import DisruptionRiskModel\n",
    "\n",
    "# Reduce optuna trials for notebook speed; increase to 200 for production\n",
    "model = DisruptionRiskModel(n_optuna_trials=30, calibrate=True)\n",
    "metrics = model.train(train_df)\n",
    "\n",
    "print(f'\\nBest params: {metrics.best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    roc_curve, precision_recall_curve, roc_auc_score,\n",
    "    average_precision_score, brier_score_loss, calibration_curve\n",
    ")\n",
    "\n",
    "y_test  = test_df[TARGET].values\n",
    "y_prob  = model.predict_proba(test_df)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "axes[0].plot(fpr, tpr, lw=2, color='#2563eb', label=f'AUC = {auc:.4f}')\n",
    "axes[0].plot([0,1],[0,1], 'k--', lw=0.8)\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve — Disruption Risk Model')\n",
    "axes[0].legend()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "ap = average_precision_score(y_test, y_prob)\n",
    "axes[1].plot(rec, prec, lw=2, color='#16a34a', label=f'AP = {ap:.4f}')\n",
    "axes[1].axhline(y_test.mean(), color='gray', lw=0.8, linestyle='--', label='Baseline')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].legend()\n",
    "\n",
    "# Calibration Curve\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n",
    "axes[2].plot(prob_pred, prob_true, 's-', lw=2, color='#dc2626', label='XGB+Platt')\n",
    "axes[2].plot([0,1],[0,1], 'k--', lw=0.8, label='Perfect calibration')\n",
    "brier = brier_score_loss(y_test, y_prob)\n",
    "axes[2].set_xlabel('Mean Predicted Probability')\n",
    "axes[2].set_ylabel('Fraction of Positives')\n",
    "axes[2].set_title(f'Calibration Curve (Brier={brier:.4f})')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/disruption_model_evaluation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Test AUC: {auc:.4f} | AP: {ap:.4f} | Brier: {brier:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SHAP Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = model.explainer\n",
    "X_test_feat = test_df[FEATURES].fillna(0).astype(float)\n",
    "shap_values = explainer.shap_values(X_test_feat)\n",
    "\n",
    "# Mean |SHAP| per feature\n",
    "shap_importance = pd.DataFrame({\n",
    "    'feature':       FEATURES,\n",
    "    'mean_abs_shap': np.abs(shap_values).mean(axis=0)\n",
    "}).sort_values('mean_abs_shap', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "bars = ax.barh(shap_importance['feature'], shap_importance['mean_abs_shap'],\n",
    "               color='#2563eb', edgecolor='white', alpha=0.85)\n",
    "ax.set_xlabel('Mean |SHAP Value| — Feature Importance')\n",
    "ax.set_title('SHAP TreeExplainer — Global Feature Attribution\\n'\n",
    "             'DisruptionRiskModel (XGBoost + Optuna HPO)')\n",
    "ax.set_xlim(0, shap_importance['mean_abs_shap'].max() * 1.15)\n",
    "\n",
    "# Annotate top 5\n",
    "top5 = shap_importance.tail(5)\n",
    "for bar, val in zip(bars[-5:], top5['mean_abs_shap']):\n",
    "    ax.text(val + 0.001, bar.get_y() + bar.get_height()/2,\n",
    "            f'{val:.4f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/shap_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Top 5 disruption risk drivers:')\n",
    "print(shap_importance.tail(5)[['feature','mean_abs_shap']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Anomaly Detector Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.anomaly_detector import SupplyChainAnomalyDetector, ANOMALY_FEATURES\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Inject anomalies into test set for evaluation\n",
    "test_anom = test_df.copy()\n",
    "n_anom = int(len(test_anom) * 0.12)\n",
    "anom_idx = np.random.choice(len(test_anom), n_anom, replace=False)\n",
    "test_anom.iloc[anom_idx, test_anom.columns.get_loc('disruption_rate_90d')] = \\\n",
    "    np.random.uniform(0.6, 1.0, n_anom)\n",
    "test_anom.iloc[anom_idx, test_anom.columns.get_loc('avg_delay_days_30d')] = \\\n",
    "    np.random.uniform(15, 30, n_anom)\n",
    "test_anom.iloc[anom_idx, test_anom.columns.get_loc('on_time_rate_30d')] = \\\n",
    "    np.random.uniform(0.0, 0.25, n_anom)\n",
    "anom_labels = np.zeros(len(test_anom)); anom_labels[anom_idx] = 1\n",
    "\n",
    "detector = SupplyChainAnomalyDetector(contamination=0.05, ensemble_threshold=0.60)\n",
    "detector.fit(train_df[ANOMALY_FEATURES + ['supplier_id']])\n",
    "scored = detector.score(test_anom)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(anom_labels, scored['anomaly_score_ensemble'])\n",
    "auc = roc_auc_score(anom_labels, scored['anomaly_score_ensemble'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "axes[0].plot(fpr, tpr, lw=2, color='#9333ea', label=f'Ensemble AUC={auc:.3f}')\n",
    "axes[0].plot([0,1],[0,1],'k--',lw=0.8)\n",
    "axes[0].set_xlabel('FPR'); axes[0].set_ylabel('TPR')\n",
    "axes[0].set_title('Anomaly Detector — ROC Curve\\n(IF + ECOD Ensemble)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Score distribution\n",
    "axes[1].hist(scored.loc[anom_labels==0,'anomaly_score_ensemble'],\n",
    "             bins=30, alpha=0.65, color='#2563eb', label='Normal')\n",
    "axes[1].hist(scored.loc[anom_labels==1,'anomaly_score_ensemble'],\n",
    "             bins=30, alpha=0.65, color='#dc2626', label='Anomaly')\n",
    "axes[1].axvline(0.60, color='k', lw=1.5, linestyle='--', label='Threshold=0.60')\n",
    "axes[1].set_xlabel('Ensemble Anomaly Score')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Score Distributions — Normal vs Anomaly')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/anomaly_detector_evaluation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Anomaly Detector AUC: {auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Monte Carlo VaR — Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulation.monte_carlo_var import MonteCarloVaR\n",
    "\n",
    "N_SUP, N_SKU = 15, 30\n",
    "\n",
    "suppliers = pd.DataFrame({\n",
    "    'supplier_id':              [f'SUP-{i:02d}' for i in range(N_SUP)],\n",
    "    'disruption_probability':   np.random.uniform(0.05, 0.55, N_SUP),\n",
    "    'annual_spend_usd':         np.random.uniform(500_000, 10_000_000, N_SUP),\n",
    "    'avg_lead_time_days':       np.random.uniform(5, 45, N_SUP),\n",
    "    'substitution_cost_factor': np.random.uniform(1.1, 3.0, N_SUP),\n",
    "})\n",
    "inventory = pd.DataFrame({\n",
    "    'sku_id':            [f'SKU-{i:03d}' for i in range(N_SKU)],\n",
    "    'supplier_id':       [f'SUP-{i % N_SUP:02d}' for i in range(N_SKU)],\n",
    "    'days_of_supply':    np.random.uniform(3, 30, N_SKU),\n",
    "    'avg_daily_revenue': np.random.uniform(5_000, 100_000, N_SKU),\n",
    "    'demand_p10':        np.random.uniform(80, 100, N_SKU),\n",
    "    'demand_p50':        np.random.uniform(100, 120, N_SKU),\n",
    "    'demand_p90':        np.random.uniform(120, 150, N_SKU),\n",
    "})\n",
    "base_corr = np.eye(N_SUP)*0.8 + np.ones((N_SUP,N_SUP))*0.2\n",
    "np.fill_diagonal(base_corr, 1.0)\n",
    "contagion = np.random.uniform(0, 0.3, (N_SUP,N_SUP))\n",
    "np.fill_diagonal(contagion, 0)\n",
    "\n",
    "# Convergence: VaR(95) as function of simulation count\n",
    "sim_counts = [1_000, 2_500, 5_000, 10_000, 25_000, 50_000]\n",
    "var_95s, cvar_95s = [], []\n",
    "\n",
    "for n in sim_counts:\n",
    "    sim = MonteCarloVaR(n_simulations=n, random_seed=42)\n",
    "    res = sim.run(suppliers, inventory, contagion, base_corr)\n",
    "    var_95s.append(res.var_95)\n",
    "    cvar_95s.append(res.cvar_95)\n",
    "    print(f'  N={n:>7,} | VaR(95%)=${res.var_95/1e6:.3f}M | CVaR(95%)=${res.cvar_95/1e6:.3f}M')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.semilogx(sim_counts, [v/1e6 for v in var_95s],  's-', color='#2563eb', lw=2, label='VaR(95%)')\n",
    "ax.semilogx(sim_counts, [c/1e6 for c in cvar_95s], 'D-', color='#dc2626', lw=2, label='CVaR(95%)')\n",
    "ax.axvline(50_000, color='gray', lw=1, linestyle='--', label='Production: N=50,000')\n",
    "ax.set_xlabel('Number of Monte Carlo Simulations (log scale)')\n",
    "ax.set_ylabel('Value at Risk ($M)')\n",
    "ax.set_title('Monte Carlo VaR Convergence Analysis\\n'\n",
    "             'Gaussian Copula + Network Contagion Propagation')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/var_convergence.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Supplier Risk Scoring — Top Risk Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_suppliers = model.score_suppliers(test_df)\n",
    "\n",
    "# Risk tier distribution\n",
    "tier_counts = scored_suppliers['risk_tier'].value_counts()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "# Score histogram\n",
    "axes[0].hist(scored_suppliers['disruption_probability'], bins=40,\n",
    "             color='#2563eb', alpha=0.75, edgecolor='white')\n",
    "axes[0].axvline(0.25, color='#16a34a', lw=1.5, linestyle='--', label='LOW|MEDIUM (0.25)')\n",
    "axes[0].axvline(0.50, color='#f59e0b', lw=1.5, linestyle='--', label='MEDIUM|HIGH (0.50)')\n",
    "axes[0].axvline(0.75, color='#dc2626', lw=1.5, linestyle='--', label='HIGH|CRITICAL (0.75)')\n",
    "axes[0].set_xlabel('Disruption Probability P(t+30)')\n",
    "axes[0].set_ylabel('Supplier Count')\n",
    "axes[0].set_title('Disruption Probability Distribution')\n",
    "axes[0].legend(fontsize=9)\n",
    "\n",
    "# Risk tier pie\n",
    "colors = {'LOW':'#16a34a','MEDIUM':'#f59e0b','HIGH':'#f97316','CRITICAL':'#dc2626'}\n",
    "tier_order = ['LOW','MEDIUM','HIGH','CRITICAL']\n",
    "vals = [tier_counts.get(t, 0) for t in tier_order]\n",
    "axes[1].pie(vals, labels=tier_order, colors=[colors[t] for t in tier_order],\n",
    "            autopct='%1.1f%%', startangle=90, pctdistance=0.75)\n",
    "axes[1].set_title('Supplier Risk Tier Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/risk_tier_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\nTop 10 Highest-Risk Suppliers:')\n",
    "top10 = scored_suppliers.head(10)[['supplier_id','disruption_probability','risk_tier','rank']]\n",
    "display(top10.style.background_gradient(subset=['disruption_probability'],\n",
    "                                        cmap='RdYlGn_r').format({'disruption_probability':'{:.3f}'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n=== Validation Complete ===')\n",
    "print(f'DisruptionRiskModel  | Test AUC: {auc:.4f}')\n",
    "print(f'AnomalyDetector      | AUC vs injected anomalies: {auc:.4f}')\n",
    "print(f'MonteCarloVaR        | Converged at N=50,000 simulations')\n",
    "print(f'SHAP Attribution     | Top driver: {shap_importance.iloc[-1].feature}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
